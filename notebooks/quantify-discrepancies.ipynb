{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantification of the prominence of discrepancies in ML models in the data science practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyemd import emd_samples\n",
    "\n",
    "from sklearn.metrics import f1_score, plot_precision_recall_curve, RocCurveDisplay, plot_confusion_matrix, accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import appdirs\n",
    "\n",
    "import openml\n",
    "from mltasks import openml_tasks\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "\n",
    "PATH_DATA = appdirs.user_cache_dir(\"mltasks\", \"mltasks\")\n",
    "PATH_OPENML = PATH_DATA+'/openml/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data from OpenML\n",
    "##### Input/output data, predictions from best models submitted to OpenML and computation of prediction discrepancies between those models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tables import NaturalNameWarning\n",
    "warnings.filterwarnings('ignore', category=NaturalNameWarning)\n",
    "\n",
    "# OpenML-CC18 Curated Classification benchmark\n",
    "suite = openml_tasks.get_suite(suite='OpenML-CC18')\n",
    "\n",
    "for task_id in suite.tasks[28:]:\n",
    "    (discr, preds, scores) = openml_tasks.get_discrepancies(task=task_id, metric='predictive_accuracy', n_runs=100, epsilon_runs=0.02)\n",
    "    (data, target) = openml_tasks.get_dataset(task=task_id)\n",
    "    #openml_fetcher.get_discr(task_id, get_data=False, path=path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of datasets retrieved from OpenML for the benchmark\n",
    "with pd.HDFStore(PATH_OPENML+'/openml-discr.h5') as store:\n",
    "    print( len( list(store.keys()) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive analysis of prediction discrepancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_discr = pd.HDFStore(PATH_OPENML+'/openml-discr.h5')\n",
    "store_accuracies = pd.HDFStore(PATH_OPENML+'/openml-accuracies.h5')\n",
    "\n",
    "df_dataset_properties = {}\n",
    "\n",
    "for dataset_id in list(store_discr.keys()):\n",
    "\n",
    "    discr = store_discr[dataset_id]\n",
    "    prediction_error = 1-store_accuracies[dataset_id]\n",
    "\n",
    "    dataset = openml.datasets.get_dataset(int(dataset_id.split('/')[1]))\n",
    "    df_dataset_properties[dataset.name] = {'Proportion of discrepancies':discr.sum()/discr.shape[0],\n",
    "                        'Prediction error of the worst model':prediction_error.max(),\n",
    "                        'Number of instances':dataset.qualities['NumberOfInstances'],\n",
    "                        'Number of features':dataset.qualities['NumberOfFeatures'],\n",
    "                        'Ratio features/instances':dataset.qualities['NumberOfFeatures']/float(dataset.qualities['NumberOfInstances'])}\n",
    "\n",
    "df_dataset_properties = pd.DataFrame(df_dataset_properties).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dataset_properties.columns)\n",
    "\n",
    "df_dataset_properties.plot(kind='scatter', y='Prediction error of the worst model', x='Proportion of discrepancies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of the number of prediction discrepancies by dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "ax = sns.boxplot(data=df_dataset_properties, y='Proportion of discrepancies',\n",
    "            whis=[0, 100], width=.6, palette=\"vlag\")\n",
    "\n",
    "sns.stripplot(data=df_dataset_properties, y='Proportion of discrepancies',\n",
    "              size=4, color=\".3\", linewidth=0)\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.title('Proportion of instances $\\\\bf{with\\ prediction\\ discrepancies}$\\n over the 72 datasets of OpenML-CC18')\n",
    "plt.ylabel('')\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.expanduser('~')+'/Desktop/discrepancies/figures/proportion_with_discr.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_dataset_properties.loc[:,'Proportion of discrepancies']\n",
    "\n",
    "s = s.round(3)*100\n",
    "s.name = \"Proportion of predictions with discrepancies\"\n",
    "\n",
    "for i in range(s.shape[0]):\n",
    "    s.iloc[i] = str(s.iloc[i])+'%'\n",
    "\n",
    "print(s.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the structure of instances with prediction discrepancies with the Wasserstein distance\n",
    "\n",
    "The dataset needs first to be prepared in order to compute the Wasserstein distance between instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "\n",
    "def feature_encoder(X, feature_to_encode):\n",
    "    \"\"\"\n",
    "    Encode non-numeric features, remove the original feature and concatenate its encoded version in the dataset returned\n",
    "\n",
    "    Args:\n",
    "        X ([type]): dataset\n",
    "        feature_to_encode (string or int): feature to encore (name of the dataframe column)\n",
    "    \"\"\"\n",
    "    dummies = pd.get_dummies( X.loc[:,[feature_to_encode]] )\n",
    "    res = pd.concat([X.drop(labels=feature_to_encode, axis=1), dummies], axis=1)\n",
    "\n",
    "    return(res) \n",
    "\n",
    "def get_and_prepare_openML_dataset(dataset_id):\n",
    "\n",
    "    # Get OpenML dataset properties\n",
    "    dataset = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "    # Retrieve dataset\n",
    "    (data, y) = openml_tasks.get_dataset(task=dataset_id)\n",
    "    X = data\n",
    "\n",
    "    # Encode non-numeric features\n",
    "    features_to_encode = dataset.get_features_by_type('nominal')\n",
    "    features_to_encode = [X.columns[feature] for feature in features_to_encode[:-1]]\n",
    "    for feature in features_to_encode:\n",
    "        X = feature_encoder(X, feature)\n",
    "\n",
    "    # Complete missing values\n",
    "    X = KNNImputer(n_neighbors=1).fit_transform(X)\n",
    "\n",
    "    # Standardize features\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X, index=data.index)\n",
    "\n",
    "    # Encode target\n",
    "    y = LabelEncoder().fit_transform(y.to_frame())\n",
    "    y = y.flatten()\n",
    "    y = pd.Series(y, index=X.index)\n",
    "\n",
    "    # Retrieve pre-computed discrepancies\n",
    "    with pd.HDFStore(PATH_OPENML+'/openml-discr.h5') as store:\n",
    "        y_discr = store[str(dataset_id)]\n",
    "    mask_instances_with_discrepancies = (y_discr==1).values\n",
    "\n",
    "    y.name = 'Label'\n",
    "    y_discr.name = 'Discrepancies'\n",
    "\n",
    "    labels = pd.concat((y, y_discr), axis=1)\n",
    "\n",
    "    return (X, labels, mask_instances_with_discrepancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of the dataset' cleaning & preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataset_id = 3\n",
    "X, labels, mask_instances_with_discrepancies = get_and_prepare_openML_dataset(dataset_id)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "cross_val_score(clf, X, labels.Label).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "# OpenML-CC18 Curated Classification benchmark\n",
    "suite = openml_tasks.get_suite(suite='OpenML-CC18')\n",
    "\n",
    "for task_id in suite.tasks[:]:\n",
    "\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    dataset_id = task.dataset_id\n",
    "\n",
    "    X, labels, mask_instances_with_discrepancies = get_and_prepare_openML_dataset(dataset_id)\n",
    "\n",
    "    if len(labels.Label.unique())>2:\n",
    "            continue\n",
    "\n",
    "    print('#########')\n",
    "    print(dataset_id)\n",
    "\n",
    "    c = labels.Label.unique()[0]\n",
    "    if len(X[labels.Label==c][labels.Discrepancies==1])==0 or len(X[labels.Label!=c][labels.Discrepancies==1])==0:\n",
    "        print(\"not enough discrepancies\")\n",
    "        continue\n",
    "\n",
    "    n_samples = int(X.shape[0]/2)\n",
    "    print(n_samples)\n",
    "    dist0 = emd_samples(X.sample(n_samples), X.sample(n_samples))\n",
    "    res.append({'Dataset':dataset_id, 'Distance':dist0, 'Comparison':'Entire dataset', 'Label':None})\n",
    "\n",
    "    # Distance computations are made 1 class versus another class\n",
    "\n",
    "    for c in labels.Label.unique():\n",
    "\n",
    "        tmp1 = X[labels.Label==c].values\n",
    "        tmp2 = X[labels.Label!=c].values\n",
    "        dist1 = emd_samples(tmp1, tmp2)\n",
    "        res.append({'Dataset':dataset_id, 'Distance':dist1, 'Comparison':'Between classes', 'Label':str(c)+' vs all'})\n",
    "\n",
    "        tmp1 = X[labels.Label==c][labels.Discrepancies==0].values\n",
    "        tmp2 = X[labels.Label!=c][labels.Discrepancies==0].values\n",
    "        dist2 = emd_samples(tmp1, tmp2)\n",
    "        res.append({'Dataset':dataset_id, 'Distance':dist2, 'Comparison':'Between classes - Instances without discrepancies', 'Label':str(c)+' vs all'})\n",
    "\n",
    "        tmp1 = X[labels.Label==c][labels.Discrepancies==1].values\n",
    "        tmp2 = X[labels.Label!=c][labels.Discrepancies==1].values\n",
    "        dist3 = emd_samples(tmp1, tmp2)\n",
    "        res.append({'Dataset':dataset_id, 'Distance':dist3, 'Comparison':'Between classes - Instances with discrepancies', 'Label':str(c)+' vs all'})\n",
    "\n",
    "        res.append({'Dataset':dataset_id, 'Distance':dist2/dist3, 'Comparison':'Ratio', 'Label':str(c)+' vs all'})\n",
    "\n",
    "    df = pd.DataFrame(res)\n",
    "    df.to_csv(os.path.expanduser('~')+'/Desktop/discrepancies/stats_discr.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv(os.path.expanduser('~')+'/Desktop/discrepancies/stats_discr.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.loc[:,'Distance'].groupby(res.Comparison).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,5)) \n",
    "sns.boxplot(data=res[res.Comparison!='Ratio'], x='Distance', y='Comparison', palette=\"vlag\", ax=ax)\n",
    "ax.set_yticklabels(['Distance between 2 random samples\\n from the entire dataset - $\\\\bf{for\\ control}$',\n",
    "                     'Distances between instances of\\n opposite classes - $\\\\bf{for\\ control}$',\n",
    "                     'Distances between instances of\\n opposite classes $\\\\bf{without\\ discrepancies}$',\n",
    "                     'Distances between instances of\\n opposite classes $\\\\bf{with\\ discrepancies}$'], rotation=0, horizontalalignment='right')\n",
    "#ax.set_title(\"Distributions of normalized Wasserstein distances for various\\n configurations over OpenML-CC18 binary classification datasets\")\n",
    "ax.set_title(\"Comparison of the closeness of instances\\n with and without prediction discrepancies\")\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('Distribution of Wasserstein distances over \\nOpenML-CC18 binary classification datasets')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.expanduser('~')+'/Desktop/discrepancies/figures/wasserstein.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE, Isomap\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_embedded = PCA(n_components=2).fit_transform(X)\n",
    "X_embedded = pd.DataFrame(X_embedded, index=X.index)\n",
    "\n",
    "df_tmp = pd.concat((X_embedded, labels), axis=1)\n",
    "\n",
    "# scatterplot\n",
    "sns.scatterplot(data=df_tmp, x=0, y=1, hue=\"Discrepancies\", style=\"Label\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "ae1fefc8646a06dd2e75004cd934adda7c5727b046986a772e3b44b0ffba9754"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
