{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantification of the prominence of discrepancies in ML models in the data science practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mltasks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w8/jpxzynl13791vcqvfpfk9brc0000gp/T/ipykernel_41433/261570396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmltasks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenml_tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mltasks'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyemd import emd_samples\n",
    "\n",
    "from sklearn.metrics import f1_score, plot_precision_recall_curve, RocCurveDisplay, plot_confusion_matrix, accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import openml\n",
    "import openml_fetcher\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "\n",
    "path_data = '/Users/---/Desktop/discrepancies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch OpenML data (accuracy scores, predictions, compute prediction discrepancies)\n",
    "\n",
    "# OpenML-CC18 Curated Classification benchmark\n",
    "suite = openml.study.get_suite(99)\n",
    "\n",
    "for task_id in suite.tasks:\n",
    "    openml_fetcher.get_discr(task_id, get_data=False, path=path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the number of datasets retrieved from OpenML for the benchmark\n",
    "with pd.HDFStore(path_data+'/openml/openml-discr.h5') as store:\n",
    "    print( len( list(store.keys()) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_discr = pd.HDFStore(path_data+'/openml/openml-discr.h5')\n",
    "store_accuracies = pd.HDFStore(path_data+'/openml/openml-accuracies.h5')\n",
    "\n",
    "df_dataset_properties = {}\n",
    "\n",
    "for dataset_id in list(store_discr.keys()):\n",
    "\n",
    "    discr = store_discr[dataset_id]\n",
    "    prediction_error = 1-store_accuracies[dataset_id]\n",
    "\n",
    "    dataset = openml.datasets.get_dataset(int(dataset_id.split('/')[1]))\n",
    "    df_dataset_properties[dataset.name] = {'Proportion of discrepancies':discr.sum()/discr.shape[0],\n",
    "                        'Prediction error of the worst model':prediction_error.max(),\n",
    "                        'Number of instances':dataset.qualities['NumberOfInstances'],\n",
    "                        'Number of features':dataset.qualities['NumberOfFeatures'],\n",
    "                        'Ratio features/instances':dataset.qualities['NumberOfFeatures']/float(dataset.qualities['NumberOfInstances'])}\n",
    "\n",
    "df_dataset_properties = pd.DataFrame(df_dataset_properties).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dataset_properties.columns)\n",
    "\n",
    "df_dataset_properties.plot(kind='scatter', y='Prediction error of the worst model', x='Proportion of discrepancies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_dataset_properties.loc[:,'Proportion of discrepancies']\n",
    "np.sum(tmp>=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "ax = sns.boxplot(data=df_dataset_properties, y='Proportion of discrepancies',\n",
    "            whis=[0, 100], width=.6, palette=\"vlag\")\n",
    "\n",
    "sns.stripplot(data=df_dataset_properties, y='Proportion of discrepancies',\n",
    "              size=4, color=\".3\", linewidth=0)\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.title('Proportion of instances $\\\\bf{with\\ prediction\\ discrepancies}$\\n over the 72 datasets of OpenML-CC18')\n",
    "plt.ylabel('')\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_data+'/figures/proportion_with_discr.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_dataset_properties.loc[:,'Proportion of discrepancies']\n",
    "\n",
    "s = s.round(3)*100\n",
    "s.name = \"Proportion of predictions with discrepancies\"\n",
    "\n",
    "for i in range(s.shape[0]):\n",
    "    s.iloc[i] = str(s.iloc[i])+'%'\n",
    "\n",
    "print(s.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "\n",
    "def feature_encoder(X, feature_to_encode):\n",
    "    \"\"\"\n",
    "    Encode non-numeric features, remove the original feature and concatenate its encoded version in the dataset returned\n",
    "\n",
    "    Args:\n",
    "        X ([type]): dataset\n",
    "        feature_to_encode (string or int): feature to encore (name of the dataframe column)\n",
    "    \"\"\"\n",
    "    dummies = pd.get_dummies( X.loc[:,[feature_to_encode]] )\n",
    "    res = pd.concat([X.drop(labels=feature_to_encode, axis=1), dummies], axis=1)\n",
    "\n",
    "    return(res) \n",
    "\n",
    "def get_and_prepare_openML_dataset(dataset_id):\n",
    "\n",
    "    # Get OpenML dataset properties\n",
    "    dataset = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "    # Retrieve dataset\n",
    "    (data, y) = fetch_openml(data_id=dataset_id, return_X_y=True)\n",
    "    X = data\n",
    "\n",
    "    # Encode non-numeric features\n",
    "    features_to_encode = dataset.get_features_by_type('nominal')\n",
    "    features_to_encode = [X.columns[feature] for feature in features_to_encode[:-1]]\n",
    "    for feature in features_to_encode:\n",
    "        X = feature_encoder(X, feature)\n",
    "\n",
    "    # Complete missing values\n",
    "    X = KNNImputer(n_neighbors=1).fit_transform(X)\n",
    "\n",
    "    # Standardize features\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X, index=data.index)\n",
    "\n",
    "    # Encode target\n",
    "    y = LabelEncoder().fit_transform(y.to_frame())\n",
    "    y = y.flatten()\n",
    "    y = pd.Series(y, index=X.index)\n",
    "\n",
    "    # Retrieve pre-computed discrepancies\n",
    "    with pd.HDFStore(path_data+'/openml/openml-discr.h5') as store:\n",
    "        y_discr = store[str(dataset_id)]\n",
    "    mask_instances_with_discrepancies = (y_discr==1).values\n",
    "\n",
    "    y.name = 'Label'\n",
    "    y_discr.name = 'Discrepancies'\n",
    "\n",
    "    labels = pd.concat((y, y_discr), axis=1)\n",
    "\n",
    "    return (X, labels, mask_instances_with_discrepancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of the dataset' cleaning & preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataset_id = 15\n",
    "X, labels, mask_instances_with_discrepancies = get_and_prepare_openML_dataset(dataset_id)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "cross_val_score(clf, X, labels.Label).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "# OpenML-CC18 Curated Classification benchmark\n",
    "suite = openml.study.get_suite(99)\n",
    "\n",
    "for task_id in suite.tasks[:]:\n",
    "\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    dataset_id = task.dataset_id\n",
    "\n",
    "    X, labels, mask_instances_with_discrepancies = get_and_prepare_openML_dataset(dataset_id)\n",
    "\n",
    "    if len(labels.Label.unique())>2:\n",
    "            continue\n",
    "\n",
    "    print('#########')\n",
    "    print(dataset_id)\n",
    "\n",
    "    c = labels.Label.unique()[0]\n",
    "    if len(X[labels.Label==c][labels.Discrepancies==1])==0 or len(X[labels.Label!=c][labels.Discrepancies==1])==0:\n",
    "        print(\"not enough discrepancies\")\n",
    "        continue\n",
    "\n",
    "    n_samples = int(X.shape[0]/2)\n",
    "    print(n_samples)\n",
    "    dist0 = emd_samples(X.sample(n_samples), X.sample(n_samples))\n",
    "    res.append({'Dataset':dataset_id, 'Distance':dist0, 'Comparison':'Entire dataset', 'Label':None})\n",
    "\n",
    "    # Distance computations are made 1 class versus another class\n",
    "\n",
    "    for c in labels.Label.unique():\n",
    "\n",
    "        tmp1 = X[labels.Label==c].values\n",
    "        tmp2 = X[labels.Label!=c].values\n",
    "        dist1 = emd_samples(tmp1, tmp2)\n",
    "        res.append({'Dataset':dataset_id, 'Distance':dist1, 'Comparison':'Between classes', 'Label':str(c)+' vs all'})\n",
    "\n",
    "        tmp1 = X[labels.Label==c][labels.Discrepancies==0].values\n",
    "        tmp2 = X[labels.Label!=c][labels.Discrepancies==0].values\n",
    "        dist2 = emd_samples(tmp1, tmp2)\n",
    "        res.append({'Dataset':dataset_id, 'Distance':dist2, 'Comparison':'Between classes - Instances without discrepancies', 'Label':str(c)+' vs all'})\n",
    "\n",
    "        tmp1 = X[labels.Label==c][labels.Discrepancies==1].values\n",
    "        tmp2 = X[labels.Label!=c][labels.Discrepancies==1].values\n",
    "        dist3 = emd_samples(tmp1, tmp2)\n",
    "        res.append({'Dataset':dataset_id, 'Distance':dist3, 'Comparison':'Between classes - Instances with discrepancies', 'Label':str(c)+' vs all'})\n",
    "\n",
    "        res.append({'Dataset':dataset_id, 'Distance':dist2/dist3, 'Comparison':'Ratio', 'Label':str(c)+' vs all'})\n",
    "\n",
    "    df = pd.DataFrame(res)\n",
    "    df.to_csv(path_data+'/openml/stats_discr.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/Users/a435vv/Desktop/discrepancies'\n",
    "res = pd.read_csv(path_data+'/openml/stats_discr.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.loc[:,'Distance'].groupby(res.Comparison).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,5)) \n",
    "sns.boxplot(data=res[res.Comparison!='Ratio'], x='Distance', y='Comparison', palette=\"vlag\", ax=ax)\n",
    "ax.set_yticklabels(['Distance between 2 random samples\\n from the entire dataset - $\\\\bf{for\\ control}$',\n",
    "                     'Distances between instances of\\n opposite classes - $\\\\bf{for\\ control}$',\n",
    "                     'Distances between instances of\\n opposite classes $\\\\bf{without\\ discrepancies}$',\n",
    "                     'Distances between instances of\\n opposite classes $\\\\bf{with\\ discrepancies}$'], rotation=0, horizontalalignment='right')\n",
    "#ax.set_title(\"Distributions of normalized Wasserstein distances for various\\n configurations over OpenML-CC18 binary classification datasets\")\n",
    "ax.set_title(\"Comparison of the closeness of instances\\n with and without prediction discrepancies\")\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('Distribution of Wasserstein distances over \\nOpenML-CC18 binary classification datasets')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_data+'/figures/wasserstein.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE, Isomap\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_embedded = PCA(n_components=2).fit_transform(X)\n",
    "X_embedded = pd.DataFrame(X_embedded, index=X.index)\n",
    "\n",
    "df_tmp = pd.concat((X_embedded, labels), axis=1)\n",
    "\n",
    "# scatterplot\n",
    "sns.scatterplot(data=df_tmp, x=0, y=1, hue=\"Discrepancies\", style=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look what labels / discrepancies have the nearest neighbours of points with discrepancies\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=11).fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def count_discr(x):\n",
    "    return df.Discrepancies.iloc[x].sum()\n",
    "\n",
    "def count_label(x):\n",
    "    return df.Label.iloc[x].value_counts()\n",
    "\n",
    "nbrs_discr = pd.DataFrame(indices[:,1:]).apply(count_discr, axis=1)\n",
    "nbrs_discr = nbrs_discr/indices[:,1:].shape[1]\n",
    "nbrs_discr.name = 'Proportion of discrepancies among neighbours'\n",
    "\n",
    "nbrs_labels = pd.DataFrame(indices[:,1:]).apply(count_label, axis=1).fillna(0)\n",
    "nbrs_labels = nbrs_labels/indices[:,1:].shape[1]\n",
    "nbrs_labels.columns = ['Label '+str(c) for c in nbrs_labels.columns]\n",
    "\n",
    "nbrs_prop_opposite_labels = []\n",
    "for i in range(X.shape[0]):\n",
    "    nbrs_prop_opposite_labels.append ( (y.iloc[indices[i,1:]] != y.iloc[i]).astype(int).sum() )\n",
    "nbrs_prop_opposite_labels = pd.Series(nbrs_prop_opposite_labels, index=X.index, name='Proportion of neighbours with different labels')\n",
    "nbrs_prop_opposite_labels = nbrs_prop_opposite_labels/indices[:,1:].shape[1]\n",
    "\n",
    "# Mean distance to points of opposite class / vs / same class\n",
    "X_dist = pairwise_distances(X)\n",
    "X_dist = pd.DataFrame(X_dist, index=X.index, columns=X.index)\n",
    "\n",
    "mean_dist_same_label, mean_dist_diff_label = [], []\n",
    "for i in range(X.shape[0]):\n",
    "    mean_dist_same_label.append( X_dist.loc[i, y[y==y.loc[i]].index].drop(i, errors='ignore').sort_values().iloc[:20].mean() )\n",
    "    mean_dist_diff_label.append( X_dist.loc[i, y[y!=y.loc[i]].index].drop(i, errors='ignore').sort_values().iloc[:20].mean() )\n",
    "mean_dist_same_label = pd.Series(mean_dist_same_label, index=X.index, name='Mean distance to instances with same labels')\n",
    "mean_dist_diff_label = pd.Series(mean_dist_diff_label, index=X.index, name='Mean distance to instances with different labels')\n",
    "\n",
    "df2 = pd.concat((df, nbrs_discr, nbrs_labels, nbrs_prop_opposite_labels, mean_dist_same_label, mean_dist_diff_label), axis=1)\n",
    "\n",
    "# sns.boxplot(data=df2, x=\"Discrepancies\", y=\"Proportion of discrepancies among neighbours\", whis=[0,100], width=.6, palette=\"vlag\")\n",
    "sns.displot(data=df2, hue=\"Discrepancies\", x=\"Proportion of discrepancies among neighbours\", palette=\"vlag\", kde=True, stat=\"probability\", common_norm=False)\n",
    "# sns.swarmplot(data=df2, x=\"Discrepancies\", y=\"Proportion of discrepancies among neighbours\")\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# sns.boxplot(data=df2, x=\"Discrepancies\", y=\"Proportion of neighbours with different labels\", whis=[0,100], width=.6, palette=\"vlag\")\n",
    "sns.displot(data=df2, hue=\"Discrepancies\", x=\"Proportion of neighbours with different labels\", palette=\"vlag\", kde=True, stat=\"probability\", common_norm=False)\n",
    "# sns.swarmplot(data=df2, x=\"Discrepancies\", y=\"Proportion of neighbours with different labels\")\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# sns.boxplot(data=df2, x=\"Discrepancies\", y=\"Mean distance to instances with same labels\", whis=[0,100], width=.6, palette=\"vlag\")\n",
    "sns.displot(data=df2, hue=\"Discrepancies\", x=\"Mean distance to instances with same labels\", palette=\"vlag\", kde=True, stat=\"probability\", common_norm=False)\n",
    "# sns.swarmplot(data=df2, x=\"Discrepancies\", y=\"Mean distance to instances with same labels\", palette=\"vlag\")\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# sns.boxplot(data=df2, x=\"Discrepancies\", y=\"Mean distance to instances with different labels\", whis=[0,100], width=.6, palette=\"vlag\")\n",
    "sns.displot(data=df2, hue=\"Discrepancies\", x=\"Mean distance to instances with different labels\", palette=\"vlag\", kde=True, stat=\"probability\", common_norm=False)\n",
    "# sns.swarmplot(data=df2, x=\"Discrepancies\", y=\"Mean distance to instances with different labels\", palette=\"vlag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "ax = sns.scatterplot(data=X, x=\"Proportion of discrepancies\", y=\"Prediction error of the worst model\")\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([0,1])\n",
    "plt.plot([0,1],[0,1], 'r--')\n",
    "ax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "ae1fefc8646a06dd2e75004cd934adda7c5727b046986a772e3b44b0ffba9754"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
